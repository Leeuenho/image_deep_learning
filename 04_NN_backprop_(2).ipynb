{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "04_NN_backprop (2).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Leeuenho/image_deep_learning/blob/main/04_NN_backprop_(2).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kUBTeOIwhlBj"
      },
      "source": [
        "\n",
        "## 1. 지난시간 복습\n",
        "\n",
        "Cross-entrophy loss는 다음과 같이 정의한다. \n",
        "\n",
        "$CELoss = -  \\sum^C_{i=1} (y_i \\cdot \\log \\hat{y_i})$\n",
        "\n",
        "\n",
        "여기서 C는 분류할 클래스의 개수이다. \n",
        "우리는 Loss를 모든 데이터에 기반하여 계산할 것이므로, 최종으로 구할 L는 모든 데이터 M개에 관한 CELoss의 평균을 이용한다. \n",
        "\n",
        "$L = - \\sum^{M}_{j=1} \\sum^C_{i=1} (y_i^j \\cdot \\log \\hat{y_i^j})$\n",
        "\n",
        "일반적으로 CELoss는 Softmax가 적용된 출력을 입력으로 받으며, softmax+CELoss를 입력값으로 미분하게 될 때 매우 간결한 형태의 analytic gradient를 얻는다. \n",
        "\n",
        "- $\\frac{\\partial L}{\\partial z_i} = p_i - y_i$\n",
        "\n",
        "\n",
        "- 여기서 $z_i$는 softmax에 들어가는 입력 (혹은 앞 레이어의 출력)\n",
        "\n",
        "\n",
        "\n",
        "CELoss + Softmax 결합 모듈의 미분 증명 \n",
        "\n",
        "- https://levelup.gitconnected.com/killer-combo-softmax-and-cross-entropy-5907442f60ba\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xQpj4TuSvgmn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "468ce185-089d-453c-e370-d65d38d36d0f"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# softmax function: np.exp에 너무 큰값이 들어가지 않도록 수정\n",
        "def Softmax(x):\n",
        "    e_x = np.exp(x - np.max(x))\n",
        "    return e_x / e_x.sum(axis=0)\n",
        "\n",
        "# CELoss \n",
        "def CELoss(Y_hat, labels): \n",
        "    n_data = len(labels)\n",
        "    loss = 0 \n",
        "    for i in range(n_data):        \n",
        "        celoss = np.sum(- (labels[i]*np.log(Y_hat[i])))\n",
        "        loss += celoss/n_data\n",
        "    return loss\n",
        "    \n",
        "# 데이터\n",
        "x_1 = np.array([[56, 231],\n",
        "                   [24, 1]])\n",
        "x_2 = np.array([[120, 30],\n",
        "                   [24, 0]])\n",
        "x_3 = np.array([[20, 31],\n",
        "                   [240, 100]])\n",
        "x_4 = np.array([[56, 201],\n",
        "                   [22, -10]])\n",
        "x_5 = np.array([[140, 27],\n",
        "                   [30, 10]])\n",
        "x_6 = np.array([[25, 30],\n",
        "                   [230, 110]])\n",
        "\n",
        "\n",
        "# one-hot vector 정의하기\n",
        "y_1 = np.array([1,0,0])   #label_cat\n",
        "y_2 = np.array([0,1,0])   #label_dog\n",
        "y_3 = np.array([0,0,1])   #label_ship \n",
        "y_4 = np.array([1,0,0])   #label_cat_2\n",
        "y_5 = np.array([0,1,0])   #label_dog_2\n",
        "y_6 = np.array([0,0,1])   #label_ship_2 \n",
        "\n",
        "\n",
        "#  데이터셋\n",
        "X = np.array([x_1, x_2, x_3, x_4, x_5, x_6]) # data\n",
        "Y = np.array([y_1, y_2, y_3, y_4, y_5, y_6]) # label\n",
        "\n",
        "print(X.shape)\n",
        "print(Y.shape)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(6, 2, 2)\n",
            "(6, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1y-7CwpdAEvL"
      },
      "source": [
        "## 실습과제 . 지난시간에 구현한 뉴럴넷 Class 형태로 재구현\n",
        "\n",
        "지난 시간에 구현하였던 간단한 선형분류기+Softmax+CELoss 구조를 갖는 뉴럴네트워크를 아래와 같이 python class로 구현하였다. \n",
        "\n",
        "- 구조: x -> Linear(4,3) -> Softmax -> CELoss <- y_hat\n",
        "\n",
        "여기서 Linear(in, out)은 입력in개와 출력 out개를 갖는 선형분류기를 뜻한다. Backward 함수를 구현하여 본 뉴럴넷의 학습을 동작하게 만드시오. 각 레이어에 대한 local gradient 계산법은 아래에 첨부하였다. \n",
        "\n",
        "------------------------------\n",
        "\n",
        "각 레이어에 대한 미분값\n",
        "\n",
        "Derivative of CELoss with softmax :\n",
        "\n",
        "- $\\frac{\\partial L}{\\partial x_i} = p_i - y_i$\n",
        "\n",
        "- 여기서 $x_i$는 softmax에 들어가는 입력 (혹은 앞 레이어의 출력 $z_i$)\n",
        "\n",
        "- ($L = CELoss(softmax(x), y) = CELoss(p,y)$) \n",
        "\n",
        "- https://levelup.gitconnected.com/killer-combo-softmax-and-cross-entropy-5907442f60ba\n",
        "\n",
        "Derivative of Linear layer (Y = WX):\n",
        "\n",
        "- $\\frac{\\partial L}{\\partial W} = \\frac{\\partial L}{\\partial Y} X^T$\n",
        "\n",
        "- $\\frac{\\partial L}{\\partial X} = \\frac{\\partial L}{\\partial Y} W^T$\n",
        "\n",
        "\n",
        "- http://cs231n.stanford.edu/handouts/linear-backprop.pdf\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Am8MRwZmyAPi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b71f4372-b497-40b2-9a2a-7f25f0d68fe6"
      },
      "source": [
        "# Single-layer Neural Network class\n",
        "\n",
        "class NN_1layer():\n",
        "    def __init__(self, **kwargs):\n",
        "        self.W_l1 = np.random.rand(3,4) #가중치\n",
        "        self.activation = Softmax\n",
        "        self.Loss = CELoss\n",
        "        self.learning_rate = 0.0001\n",
        "\n",
        "    def Forward(self, X):\n",
        "        # initial prediction\n",
        "        Y_hat = []\n",
        "        \n",
        "        for x in X: \n",
        "            # layer 1\n",
        "            x = np.matmul(self.W_l1, x.flatten())\n",
        "          \n",
        "            # softmax\n",
        "            p = self.activation(x)\n",
        "\n",
        "            Y_hat.append(p)\n",
        "        return Y_hat\n",
        "        \n",
        "    def getLoss(self, X, Y):\n",
        "        # y_hat -> CELoss <- y\n",
        "        Y_hat = self.Forward(X)\n",
        "        return self.Loss(Y_hat, Y)\n",
        "\n",
        "    def Backward(self, Y_hat, X, Y): \n",
        "        dW1Loss = np.zeros_like(self.W_l1)\n",
        "\n",
        "        ####################################\n",
        "        ##### 이곳에 코드를 작성하시오 #####\n",
        "        # backpropagation 구현 힌트 \n",
        "        # 1. softmax와 CELoss는 한개의 레이어로 간주하고 미분\n",
        "        #   x -> [Linear(4,3)] -> [Softmax -> CELoss] <- y_hat\n",
        "        # 2. 두 레이어의 gradient 공식은 위의 각 레이어 미분 공식을 분석\n",
        "        #   분석 시 함수, 입력, 출력 기호를 명확하게 파악할 것!\n",
        "        \n",
        "        # Backpropagation 공식\n",
        "        #   Downstream gradient = UpstreamGradient * LocalGradient\n",
        "        # - 각 레이어 별 LocalGradient 계산법을 파악\n",
        "        # - 각 레이어 별 UpstreamGradient가 무엇인지 파악\n",
        "        # - 위 두가지를 곱하여 downstreamGradient를 계산\n",
        "      \n",
        "        ##### 참고 사항. np 행렬의 형상 차이에 주의 \n",
        "        # shape (N,)은 1차원 배열로써 행렬의 transpose가 적용되지 않으므로 \n",
        "        # reshape(N,1)로 차원을 명시한 뒤에 transpose를 적용하시오. \n",
        "        # 사용 예)  x.reshape(4,1).T\n",
        "\n",
        "\n",
        "        ####################################\n",
        "        \n",
        "        up = Y_hat - Y\n",
        "        local = X.reshape(-1,4)\n",
        "        \n",
        "        z = np.matmul(up.T, local)\n",
        "        \n",
        "        dW1Loss = z\n",
        "        return dW1Loss\n",
        "\n",
        "\n",
        "\n",
        "    def training(self, X, Y, n_epoch=100):\n",
        "        iterations = 0\n",
        "        while iterations < n_epoch: \n",
        "            # gradient descent algorithm \n",
        "            Y_hat = self.Forward(X)\n",
        "            dWLoss = self.Backward(Y_hat, X, Y)\n",
        "\n",
        "            self.W_l1 = self.W_l1 - dWLoss*self.learning_rate\n",
        "            print(f'iteration: {iterations+1}, loss: {self.getLoss(X,Y)}')\n",
        "            iterations += 1\n",
        "\n",
        "\n",
        "\n",
        "my_NeuralNet = NN_1layer()\n",
        "my_NeuralNet.training(X, Y)\n",
        "\n",
        "# 학습이 잘되었다면 [1,0,0] 과 같은 형태로 출력\n",
        "#print(my_NeuralNet.Forward(X))\n",
        "\n",
        "# 학습이 잘되었다면 매우 작은값(예: 0.01) 형태가 출력된다.\n",
        "#print(my_NeuralNet.getLoss(X, Y))\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration: 1, loss: 55.125733648070806\n",
            "iteration: 2, loss: 40.86874744798082\n",
            "iteration: 3, loss: 26.749662212881145\n",
            "iteration: 4, loss: 21.661776862935756\n",
            "iteration: 5, loss: 19.761202293128406\n",
            "iteration: 6, loss: 18.23692714735504\n",
            "iteration: 7, loss: 16.74961654727891\n",
            "iteration: 8, loss: 15.207849807881935\n",
            "iteration: 9, loss: 13.807264653451103\n",
            "iteration: 10, loss: 12.253901554774897\n",
            "iteration: 11, loss: 10.85228131008338\n",
            "iteration: 12, loss: 9.31667379007588\n",
            "iteration: 13, loss: 7.916788542941438\n",
            "iteration: 14, loss: 6.369900939955491\n",
            "iteration: 15, loss: 4.969441918977458\n",
            "iteration: 16, loss: 3.4296318958373515\n",
            "iteration: 17, loss: 2.03211542895123\n",
            "iteration: 18, loss: 0.7011573011061116\n",
            "iteration: 19, loss: 0.23654702936555894\n",
            "iteration: 20, loss: 0.052075476650793144\n",
            "iteration: 21, loss: 0.03781211391007523\n",
            "iteration: 22, loss: 0.029812097635746975\n",
            "iteration: 23, loss: 0.024657924461979307\n",
            "iteration: 24, loss: 0.021049204814997776\n",
            "iteration: 25, loss: 0.018376544072051956\n",
            "iteration: 26, loss: 0.016315093470876642\n",
            "iteration: 27, loss: 0.01467535336637638\n",
            "iteration: 28, loss: 0.013339132778090906\n",
            "iteration: 29, loss: 0.012228800564056326\n",
            "iteration: 30, loss: 0.011291215978029609\n",
            "iteration: 31, loss: 0.010488752910099057\n",
            "iteration: 32, loss: 0.009794009425496674\n",
            "iteration: 33, loss: 0.00918654781518935\n",
            "iteration: 34, loss: 0.00865080934424594\n",
            "iteration: 35, loss: 0.008174737168099591\n",
            "iteration: 36, loss: 0.0077488414982928655\n",
            "iteration: 37, loss: 0.007365549527003805\n",
            "iteration: 38, loss: 0.007018743678706127\n",
            "iteration: 39, loss: 0.006703427393235968\n",
            "iteration: 40, loss: 0.006415479107167467\n",
            "iteration: 41, loss: 0.006151468392442716\n",
            "iteration: 42, loss: 0.005908516650772411\n",
            "iteration: 43, loss: 0.0056841902423116035\n",
            "iteration: 44, loss: 0.0054764175581912935\n",
            "iteration: 45, loss: 0.005283423997159884\n",
            "iteration: 46, loss: 0.005103680488632104\n",
            "iteration: 47, loss: 0.004935862376851434\n",
            "iteration: 48, loss: 0.004778816309724849\n",
            "iteration: 49, loss: 0.004631533369589683\n",
            "iteration: 50, loss: 0.004493127113622294\n",
            "iteration: 51, loss: 0.004362815507214566\n",
            "iteration: 52, loss: 0.004239905967524965\n",
            "iteration: 53, loss: 0.004123782909403538\n",
            "iteration: 54, loss: 0.004013897318052689\n",
            "iteration: 55, loss: 0.003909757973450379\n",
            "iteration: 56, loss: 0.003810924028861361\n",
            "iteration: 57, loss: 0.0037169987055770976\n",
            "iteration: 58, loss: 0.0036276239126395264\n",
            "iteration: 59, loss: 0.0035424756368828067\n",
            "iteration: 60, loss: 0.003461259977511995\n",
            "iteration: 61, loss: 0.003383709722392878\n",
            "iteration: 62, loss: 0.0033095813815674792\n",
            "iteration: 63, loss: 0.003238652608250061\n",
            "iteration: 64, loss: 0.0031707199494684095\n",
            "iteration: 65, loss: 0.003105596878177123\n",
            "iteration: 66, loss: 0.0030431120665600914\n",
            "iteration: 67, loss: 0.002983107866695958\n",
            "iteration: 68, loss: 0.00292543897008385\n",
            "iteration: 69, loss: 0.0028699712219145937\n",
            "iteration: 70, loss: 0.0028165805696268717\n",
            "iteration: 71, loss: 0.002765152128318669\n",
            "iteration: 72, loss: 0.0027155793481288553\n",
            "iteration: 73, loss: 0.0026677632708285723\n",
            "iteration: 74, loss: 0.002621611864661046\n",
            "iteration: 75, loss: 0.00257703942797836\n",
            "iteration: 76, loss: 0.0025339660535076834\n",
            "iteration: 77, loss: 0.002492317146172297\n",
            "iteration: 78, loss: 0.0024520229883172413\n",
            "iteration: 79, loss: 0.0024130183469874104\n",
            "iteration: 80, loss: 0.0023752421185847805\n",
            "iteration: 81, loss: 0.0023386370068152994\n",
            "iteration: 82, loss: 0.002303149230340139\n",
            "iteration: 83, loss: 0.002268728256982816\n",
            "iteration: 84, loss: 0.002235326561715784\n",
            "iteration: 85, loss: 0.0022028994059805594\n",
            "iteration: 86, loss: 0.0021714046361772993\n",
            "iteration: 87, loss: 0.0021408024994075572\n",
            "iteration: 88, loss: 0.0021110554747698976\n",
            "iteration: 89, loss: 0.0020821281186967845\n",
            "iteration: 90, loss: 0.0020539869229879953\n",
            "iteration: 91, loss: 0.0020266001843383995\n",
            "iteration: 92, loss: 0.001999937884290479\n",
            "iteration: 93, loss: 0.0019739715786491192\n",
            "iteration: 94, loss: 0.0019486742955019747\n",
            "iteration: 95, loss: 0.0019240204410726784\n",
            "iteration: 96, loss: 0.0018999857127111437\n",
            "iteration: 97, loss: 0.0018765470184021501\n",
            "iteration: 98, loss: 0.0018536824022235049\n",
            "iteration: 99, loss: 0.0018313709752517207\n",
            "iteration: 100, loss: 0.0018095928514521226\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BUD0Md6JLrBk"
      },
      "source": [
        "-----------------------------------------\n",
        "\n",
        "# 심화 연습문제 (optional)\n",
        "\n",
        "위 네트워크 구조를 아래와 같이 2 layer로 바꾸어 보고, 학습을 성공시켜 보시오. init(), Forward와 Backward 함수를 수정해야 한다. \n",
        "\n",
        "- 구조: x -> Linear(4,4) -> ReLU ->  Linear(4,3) -> Softmax -> CELoss <- y_hat\n",
        "\n",
        "ReLU는 아래의 구현을 참고하시오. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4aahJfjXMMtx"
      },
      "source": [
        "# ReLU\n",
        "def ReLU(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "# derivatives of ReLU\n",
        "def dReLU(x):\n",
        "    # if (x > 0):\n",
        "    #     return 1\n",
        "    # if (x <= 0):\n",
        "    #     return 0\n",
        "    return 1 * (x > 0)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gX6KovjDQJKv"
      },
      "source": [
        "####################################\n",
        "##### 이곳에 코드를 작성하시오 #####\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "####################################"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}